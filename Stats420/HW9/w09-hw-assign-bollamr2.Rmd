---
title: "Week 9 - Homework"
author: "STAT 420, Summer 2020, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---


# Directions

Students are encouraged to work together on homework. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Be sure to remove this section if you use this `.Rmd` file as a template.
- You may leave the questions in your final document.

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

## Exercise 1 (`longley` Macroeconomic Data)

The built-in dataset `longley` contains macroeconomic data for predicting employment. We will attempt to model the `Employed` variable.

```{r, eval = FALSE}
View(longley)
?longley
```

**(a)** What is the largest correlation between any pair of predictors in the dataset?

``` {r}
round(cor(longley), 2)
```

From the results above we notice that 1.00 seems to be the largest correction value between GNP and Year.


**(b)** Fit a model with `Employed` as the response and the remaining variables as predictors. Calculate and report the variance inflation factor (VIF) for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

``` {r, warning = FALSE, message = FALSE}

library(car)
ll_lm = lm(Employed ~ ., data = longley)
car::vif(ll_lm)

```

As per the results above, GNP seems to have highest VIF value with 1788.513.
In practice it is common to say that any VIF greater than 5 is cause for concern.
We notice from the results that except for Armed.Forces, all others have a VIF more than 5 and that is a multicollinearity concern.

**(c)** What proportion of the observed variation in `Population` is explained by a linear relationship with the other predictors?

``` {r}
pop_emp = lm(Population ~ . - Employed, data = longley)
summary(pop_emp)$r.squared
```

**(d)** Calculate the partial correlation coefficient for `Population` and `Employed` **with the effects of the other predictors removed**.

``` {r}
emp_pop = lm(Employed ~ . - Population, data = longley)
pop_emp = lm(Population ~ . - Employed, data = longley)

cor(resid(emp_pop), resid(pop_emp))
```

**(e)** Fit a new model with `Employed` as the response and the predictors from the model in **(b)** that were significant. (Use $\alpha = 0.05$.) Calculate and report the variance inflation factor for each of the predictors. Which variable has the largest VIF? Do any of the VIFs suggest multicollinearity?

``` {r}
#names(longley)
model = lm(Employed ~ ., data = longley)
#summary(model)
new_model = lm(Employed ~ Year + Armed.Forces + Unemployed, data = longley)
summary(new_model)

car::vif(new_model)
```
As per the results shown, largest VIF is seen for Year with a 3.891 value.
Since all the VIF values are below 5, we can say that there is no multicollinearity.


**(f)** Use an $F$-test to compare the models in parts **(b)** and **(e)**. Report the following:

- The null hypothesis
- The test statistic
- The distribution of the test statistic under the null hypothesis
- The p-value
- A decision
- Which model you prefer, **(b)** or **(e)**

``` {r}
anova(new_model, model)
```
The null hypothesis, we can notice that GNP.deflator, GNP and Population is zero.
The Test statistic values is : 1.75
F distribution there are 9 and 3 degress of freedom.
P-Value is around 0.23
Decision:- FTR at alpha = 0.05 as p-value is large and we would prefer the model in **(e)**.

**(g)** Check the assumptions of the model chosen in part **(f)**. Do any assumptions appear to be violated?

```{r, echo = FALSE}
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}
```

``` {r}
plot_fitted_resid(new_model)
plot_qq(new_model)
```
```{r, warning = FALSE, message = FALSE}
library(lmtest)
bptest(new_model)$p.value
shapiro.test(resid(new_model))$p.value
```
We notice that the residuals values aren't average around the line and so it is safe to assume that the constant variance assumption seems to be violated. As from the Normal Q-Q Plot we can see that the Normality assumption seems to be violated. But when we run the BPTest and Shapiro test, the p-values are rather large. 

Hence we can conclude that the Constant Variance assumption and Normality assumptions are not violated.


***

## Exercise 2 (`Credit` Data)

For this exercise, use the `Credit` data from the `ISLR` package. Use the following code to remove the `ID` variable which is not useful for modeling.

```{r}
library(ISLR)
data(Credit)
Credit = subset(Credit, select = -c(ID))
```

Use `?Credit` to learn about this dataset.

**(a)** Find a "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `140`
- Obtain an adjusted $R^2$ above `0.90`
- Fail to reject the Breusch-Pagan test with an $\alpha$ of $0.01$
- Use fewer than 10 $\beta$ parameters

Store your model in a variable called `mod_a`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = FALSE}
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```

``` {r}
names(Credit)
balance_all = lm(Balance ~ ., data = Credit)
summary(balance_all)
```

```{r}
#1. Selected all predictors with less than 0.3 P-Value .02
balance_7 = lm(Balance ~ Income + Limit + Rating + Cards + Age + Student, data = Credit)
summary(balance_7)
```
As a result Education, Gender, Married and Ethnicity are eliminated because of high p-value. 

```{r}
#2. Find the VIF value and eliminate the highest VIF Value predictor
vif(balance_7)
```
Limit and Rating has almost same VIF value. We can find the collinearity to eliminate one of the the two as shown below.

``` {r}
#3. Collinearity
cor(resid(lm(Balance ~ . - Rating, data = Credit)), resid(lm(Rating ~ . - Balance, data = Credit)))
cor(resid(lm(Balance ~ . - Limit, data = Credit)), resid(lm(Limit ~ . - Balance, data = Credit)))
```
As per the rule the predictor with low collinearity value get eliminated. So Limit predictor is chosen over Rating predictor.

``` {r}
#4. misc drop, clean up
balance_5 = lm(Balance ~ Income + Limit + Cards + Age + Student, data = Credit)
anova(lm(Balance ~ Income + Limit + Cards + Student, data = Credit), balance_5)
```
In the above balance_5 summary, we notice that every predictor expect Age has a much lower p-value.
Let's run anova on a model that doesn't have Age predictor and check for prefered model.

As we can notice, model with 4 parameters i.e Model1 seems to be better model in comparison.


``` {r}
#5. Figure out if we can use log values for anything.
balance_4 = lm(Balance ~ Income + Limit + Cards + Student, data = Credit)
vif(balance_4)
```
Since Income and Limit has high VIF values in comparison (though less than 5), let try log on these two predictors.

``` {r}
#6 Create models based on Log predictors.
balance_4_limit = lm(Balance ~ Income + log(Limit) + Cards + Student, data = Credit)
balance_4_income = lm(Balance ~ log(Income) + Limit + Cards + Student, data = Credit)
```
As shown above now we have 3 models with 4 predictors. Now lets try to check if there is any assumptions violations

``` {r}
#7 bptest and shiparo.test on the 3 available models
bp = c(bptest(balance_4)$p.value, bptest(balance_4_limit)$p.value, bptest(balance_4_income)$p.value)
bp

shiparo = c(shapiro.test(resid(balance_4))$p.value, shapiro.test(resid(balance_4_limit))$p.value, shapiro.test(resid(balance_4_income))$p.value)
shiparo
```
As shown above, we notice that bptest and shapiro tests suggests that balance_4_income is a better model in comparison with other models.
Same can be justfied with Norma Q-Q Plots and Residual-Fitted graphs as shown below

``` {r}
#8 Residual Fitted Graphs
par(mfrow = c(1,3))
plot_fitted_resid(balance_4)
plot_fitted_resid(balance_4_limit)
plot_fitted_resid(balance_4_income)

```
``` {r}
#9 Normal Q-Q Plots
par(mfrow = c(1,3))
plot_qq(balance_4)
plot_qq(balance_4_limit)
plot_qq(balance_4_income)
```
As concluded above, model balance_4_income seems to yield betters results in "Residual vs Fitted" graph and "Normal Q-Q" Plots

```{r}
#10 AIC Forward Search
balance_4_income_start = balance_4_income
balance_4_income_forw_aic = step(balance_4_income_start,
                                 scope = Balance ~ ((log(Income) + Limit + Cards + Student) ^2),
                                 direction = "both", trace = FALSE)
#summary(balance_4_income_forw_aic)
balance_4_income_back_aic = step(balance_4_income_start, direction = "backward", trace = FALSE)
#summary(balance_4_income_back_aic)
```
As we can notice, AIC forward search results in a model with the following parameters.
**lm(formula = Balance ~ log(Income) + Limit + Cards + Student + 
    log(Income):Limit + Limit:Student + log(Income):Student + 
    Limit:Cards, data = Credit)**

But we notice that rmse is 120.6, adj_r2 is 0.9335, decision is "Reject", and # of predictors is "9"

We are still failing in 2 parameters. Clearly further tuning is required.

``` {r}
#11 Heuristics

#remove Limit:Cards from the model. It has high p-value
mod_a = lm(formula = Balance ~ log(Income) + log(Income):Student + log(Income):Cards +
          Limit + Cards + Student, data = Credit)

vif(mod_a)
# Final Test
get_loocv_rmse(mod_a)
get_adj_r2(mod_a)
get_bp_decision(mod_a, alpha = 0.01)
get_num_params(mod_a)
```
Finally we arrive at a model 

**lm(formula = Balance ~ log(Income) + log(Income):Student + log(Income):Cards +
          Limit + Cards + Student, data = Credit)**
          
that has the following values::

- Reach a LOOCV-RMSE below 140 **132.9**
- Obtain an adjusted R2 above 0.90 **0.9193**
- Fail to reject the Breusch-Pagan test with an α of 0.01 **"Fail to Reject"**
- Use fewer than 10 β parameters **7 parameters**



**(b)** Find another "good" model for `balance` using the available predictors. Use any methods seen in class except transformations of the response. The model should:

- Reach a LOOCV-RMSE below `130`
- Obtain an adjusted $R^2$ above `0.85`
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$
- Use fewer than 25 $\beta$ parameters

Store your model in a variable called `mod_b`. Run the two given chunks to verify your model meets the requested criteria. If you cannot find a model that meets all criteria, partial credit will be given for meeting at least some of the criteria.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```

```{r, eval = FALSE}
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```

``` {r, message = FALSE, warning = FALSE}

balance_all = lm(Balance ~ ., data = Credit)
mod_a = lm(formula = Balance ~ log(Income) + log(Income):Student + log(Income):Cards +
          Limit + Cards + Student, data = Credit)
#summary(balance_all)
#summary(mod_a)
#summary(balance_4_income_forw_aic)
#summary(balance_4_income)

balance_4_income_big = lm(formula = Balance ~ (log(Income) + Limit + Cards + Student)^3, 
    data = Credit)
#summary(balance_4_income_big)
balance_4_income_BIG = lm(formula = Balance ~ (log(Income) + Limit + Cards + Student)^4, 
    data = Credit)
#summary(balance_4_income_BIG)
mod_b = balance_4_income_big
#car::vif(mod_b)

# Final Test
get_loocv_rmse(mod_b)
get_adj_r2(mod_b)
get_sw_decision(mod_b, alpha = 0.01)
get_num_params(mod_b)
```
The required criteria is met with the following the details:

- Reach a LOOCV-RMSE below `130` : **119.5**
- Obtain an adjusted $R^2$ above `0.85` : **0.9355**
- Fail to reject the Shapiro-Wilk test with an $\alpha$ of $0.01$ : **FTR**
- Use fewer than 25 $\beta$ parameters **15**

and my preferred model is  **lm(formula = Balance ~ (log(Income) + Limit + Cards + Student)^3, 
    data = Credit)**
***

## Exercise 3 (`Sacramento` Housing Data)

For this exercise, use the `Sacramento` data from the `caret` package. Use the following code to perform some preprocessing of the data.

```{r, message = FALSE, warning = FALSE}
library(caret)
library(ggplot2)
data(Sacramento)
sac_data = Sacramento
sac_data$limits = factor(ifelse(sac_data$city == "SACRAMENTO", "in", "out"))
sac_data = subset(sac_data, select = -c(city, zip))
```

Instead of using the `city` or `zip` variables that exist in the dataset, we will simply create a variable (`limits`) indicating whether or not a house is technically within the city limits of Sacramento. (We do this because they would both be factor variables with a **large** number of levels. This is a choice that is made due to laziness, not necessarily because it is justified. Think about what issues these variables might cause.)

Use `?Sacramento` to learn more about this dataset.

A plot of longitude versus latitude gives us a sense of where the city limits are.

```{r}
qplot(y = longitude, x = latitude, data = sac_data,
      col = limits, main = "Sacramento City Limits ")
```

After these modifications, we test-train split the data.

```{r}
set.seed(420)
sac_trn_idx  = sample(nrow(sac_data), size = trunc(0.80 * nrow(sac_data)))
sac_trn_data = sac_data[sac_trn_idx, ]
sac_tst_data = sac_data[-sac_trn_idx, ]
```

The training data should be used for all model fitting. Our goal is to find a model that is useful for predicting home prices.

**(a)** Find a "good" model for `price`. Use any methods seen in class. The model should reach a LOOCV-RMSE below 77,500 in the training data. Do not use any transformations of the response variable.

``` {r}
#names(sac_trn_data)
sacramento_lm = lm(price ~ ., data = sac_trn_data)
#summary(sacramento_lm)


# Drop predictor with high p-value
sacromento_p = lm(price ~ beds + sqft + longitude + type + latitude, data = sac_trn_data)

# Run Forward Search AIC
scaromento_p_full = lm(price ~ (beds + sqft + longitude + type + latitude)^2, data = sac_trn_data)
scaromento_p_full_aic = step(sacromento_p, 
                             scope = price ~ (beds + sqft + longitude + type + latitude)^2,
                             direction = "forward", 
                             trace = FALSE)
#summary(scaromento_p_full_aic)

get_loocv_rmse(sacromento_p)
get_loocv_rmse(scaromento_p_full_aic)
mod_c = scaromento_p_full_aic
```

Model **lm(price ~ beds + sqft + longitude + type + latitude, data = sac_trn_data)** gives a rmse value **77393**, which is below 77500. This model should be good enough for the question.

I also explored on the forward search aic with the model **lm(formula = price ~ beds + sqft + longitude + type + latitude + beds:longitude + beds:sqft, data = sac_trn_data)**.
AIC model gives a rmse value of **75959** way better the **sacromento_p** model.

**(b)** Is a model that achieves a LOOCV-RMSE below 77,500 useful in this case? That is, is an average error of 77,500 low enough when predicting home prices? To further investigate, use the held-out test data and your model from part **(a)** to do two things:

- Calculate the average percent error:
\[
\frac{1}{n}\sum_i\frac{|\text{predicted}_i - \text{actual}_i|}{\text{predicted}_i} \times 100
\]
- Plot the predicted versus the actual values and add the line $y = x$.

Based on all of this information, argue whether or not this model is useful.

``` {r}
predicted_values = predict(mod_c, newdata = sac_tst_data)
sum((abs(predicted_values - sac_tst_data$price)/predicted_values) * 100) / nrow(sac_trn_data)
plot(predicted_values ~ sac_tst_data$price, col = "orange")
abline(a = 0, b = 1, lwd = 2, col = "blue")
```
We notice from the plot that the predicted values are around the line suggesting that our model seems to be ok. We also notice divergence of the data past 500000. We suppose more work need to be done on the modeling to accomodate these data correctly in our model.

Average Percent error is around **6.269** which i believe to be quite low.


***

## Exercise 4 (Does It Work?)

In this exercise, we will investigate how well backwards AIC and BIC actually perform. For either to be "working" correctly, they should result in a low number of both **false positives** and **false negatives**. In model selection,

- **False Positive**, FP: Incorrectly including a variable in the model. Including a *non-significant* variable
- **False Negative**, FN: Incorrectly excluding a variable in the model. Excluding a *significant* variable

Consider the **true** model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \beta_5 x_5 + \beta_6 x_6 + \beta_7 x_7 + \beta_8 x_8 + \beta_9 x_9 + \beta_{10} x_{10} + \epsilon
\]

where $\epsilon \sim N(0, \sigma^2 = 4)$. The true values of the $\beta$ parameters are given in the `R` code below.

```{r}
beta_0  = 1
beta_1  = -1
beta_2  = 2
beta_3  = -2
beta_4  = 1
beta_5  = 1
beta_6  = 0
beta_7  = 0
beta_8  = 0
beta_9  = 0
beta_10 = 0
sigma = 2
```

Then, as we have specified them, some variables are significant, and some are not. We store their names in `R` variables for use later.

```{r}
not_sig  = c("x_6", "x_7", "x_8", "x_9", "x_10")
signif = c("x_1", "x_2", "x_3", "x_4", "x_5")
```

We now simulate values for these `x` variables, which we will use throughout part **(a)**.

```{r}
set.seed(420)
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
```

We then combine these into a data frame and simulate `y` according to the true model.

```{r}
sim_data_1 = data.frame(x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10,
  y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
)
```

We do a quick check to make sure everything looks correct.

```{r}
head(sim_data_1)
```

Now, we fit an incorrect model.

```{r}
fit = lm(y ~ x_1 + x_2 + x_6 + x_7, data = sim_data_1)
coef(fit)
```

Notice, we have coefficients for `x_1`, `x_2`, `x_6`, and `x_7`. This means that `x_6` and `x_7` are false positives, while `x_3`, `x_4`, and `x_5` are false negatives.

To detect the false negatives, use:

```{r}
# which are false negatives?
!(signif %in% names(coef(fit)))
```

To detect the false positives, use:

```{r}
# which are false positives?
names(coef(fit)) %in% not_sig
```

Note that in both cases, you could `sum()` the result to obtain the number of false negatives or positives.

**(a)** Set a seed equal to your birthday; then, using the given data for each `x` variable above in `sim_data_1`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table.

``` {r}
birthday = 19830611
set.seed(birthday)

# with new seed
n = 100
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = runif(n, 0, 10)
x_9  = runif(n, 0, 10)
x_10 = runif(n, 0, 10)
y =  rep(0, n)

# Create new sim data
sim_data_1 = data.frame(y, x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10)

# sim data run for number of simulations
num_sim = 300

# initialize
fn_aic = matrix(0, num_sim, 5)
fp_aic = matrix(0, num_sim, 5)
fn_bic = matrix(0, num_sim, 5)
fp_bic = matrix(0, num_sim, 5)
  
for (i in 1:num_sim) {
  sim_data_1$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
  
  model = lm(y ~ ., data = sim_data_1)
  model_back_aic = step(model, direction = "backward", trace = FALSE)
  model_back_bic = step(model, direction = "backward", k = log(n), trace = FALSE)
  
  fn_aic[i,] = !(signif %in% names(coef(model_back_aic))) 
  fp_aic[i,] = not_sig %in% names(coef(model_back_aic))
  fn_bic[i,] = !(signif %in% names(coef(model_back_bic)))
  fp_bic[i,] = not_sig %in% names(coef(model_back_bic))
}

rate_fn_aic = sum(fn_aic) / (num_sim * 5)
rate_fp_aic = sum(fp_aic) / (num_sim * 5)
rate_fn_bic = sum(fn_bic) / (num_sim * 5)
rate_fp_bic = sum(fp_bic) / (num_sim * 5)

# Print as a table
table = rbind(c(rate_fn_aic, rate_fp_aic), c(rate_fn_bic, rate_fp_bic))
colnames(table) = c("False Negative", "False Positive")
rownames(table) = c("AIC", "BIC")

knitr::kable(table)
```


As we can notice from the table, we have **zero** False with AIC and BIC.

BIC seems to have better rate in comparison with AIC.


**(b)** Set a seed equal to your birthday; then, using the given data for each `x` variable below in `sim_data_2`, simulate the response variable `y` 300 times. Each time,

- Fit an additive model using each of the `x` variables.
- Perform variable selection using backwards AIC.
- Perform variable selection using backwards BIC.
- Calculate and store the number of false negatives for the models chosen by AIC and BIC.
- Calculate and store the number of false positives for the models chosen by AIC and BIC.

Calculate the rate of false positives and negatives for both AIC and BIC. Compare the rates between the two methods. Arrange your results in a well formatted table. Also compare to your answers in part **(a)** and suggest a reason for any differences.

```{r}
set.seed(birthday)
x_1  = runif(n, 0, 10)
x_2  = runif(n, 0, 10)
x_3  = runif(n, 0, 10)
x_4  = runif(n, 0, 10)
x_5  = runif(n, 0, 10)
x_6  = runif(n, 0, 10)
x_7  = runif(n, 0, 10)
x_8  = x_1 + rnorm(n, 0, 0.1)
x_9  = x_1 + rnorm(n, 0, 0.1)
x_10 = x_2 + rnorm(n, 0, 0.1)
y =  rep(0, n)

# Create new sim data
sim_data_2 = data.frame(y, x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10)

# sim data run for number of simulations
num_sim = 300

# initialize
fn_aic2 = matrix(0, num_sim, 5)
fp_aic2 = matrix(0, num_sim, 5)
fn_bic2 = matrix(0, num_sim, 5)
fp_bic2 = matrix(0, num_sim, 5)
  
for (i in 1:num_sim) {
  sim_data_2$y = beta_0 + beta_1 * x_1 + beta_2 * x_2 + beta_3 * x_3 + beta_4 * x_4 + 
      beta_5 * x_5 + rnorm(n, 0 , sigma)
  
  model = lm(y ~ ., data = sim_data_2)
  model_back_aic = step(model, direction = "backward", trace = FALSE)
  model_back_bic = step(model, direction = "backward", k = log(n), trace = FALSE)
  
  fn_aic2[i,] = !(signif %in% names(coef(model_back_aic))) 
  fp_aic2[i,] = not_sig %in% names(coef(model_back_aic))
  fn_bic2[i,] = !(signif %in% names(coef(model_back_bic)))
  fp_bic2[i,] = not_sig %in% names(coef(model_back_bic))
}

rate_fn_aic2 = sum(fn_aic2) / (num_sim * 5)
rate_fp_aic2 = sum(fp_aic2) / (num_sim * 5)
rate_fn_bic2 = sum(fn_bic2) / (num_sim * 5)
rate_fp_bic2 = sum(fp_bic2) / (num_sim * 5)

# Print as a table
table = rbind(c(rate_fn_aic, rate_fp_aic, rate_fn_aic2, rate_fp_aic2), c(rate_fn_bic, rate_fp_bic, rate_fn_bic2, rate_fp_bic2))

colnames(table) = c("False Negative Sim(a)", "False Positive Sim(a)", "False Negative Sim(b)", "False Positive Sim(b)")
rownames(table) = c("AIC", "BIC")

knitr::kable(table)
```

We notice from the above table, that Simulation result b are not as good as the Simulation result a.
This occurred because of the predictors x_8, x_9, x_10. Since they are dependent on x_1 and x_2.
The task of choosing a good predictor from the data become difficult because of collinearity as it effects the predicting capabilities of each predictors.

Hence the disparity in the observed results.



