---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

# Directions

This is an **individual** project. This is NOT like a homework assignment. This is NOT an assignment where collaboration is permissible. Discussion of question intent, coding problems/issues, and project administration may be discussed on the message board on a limited basis. However, sharing, copying, or providing any part of this project to another student is an infraction of the University’s rules on academic integrity. Any violation will be punished as severely as possible.

- Your project must be submitted through Coursera. You are required to upload one `.zip` file, named `yourNetID-sim-proj.zip`, which contains:
  + Your RMarkdown file which should be saved as `yourNetID-sim-proj.Rmd`.
  + The result of knitting your RMarkdown file as `yourNetID-sim-proj.html`.
  + Any outside data provided as a `.csv` file. (In this case, `study_1.csv` and `study_2.csv`.)
- Your `.Rmd` file should be written such that, when stored in a folder with any data you are asked to import, it will knit properly without modification. If your `.zip` file is organized properly, this should not be an issue.
- Include your name and NetID in the final document, not only in your filenames.

This project consists of **three** simulation studies. Unlike a homework assignment, these "exercises" are not broken down into parts (e.g., a, b, c), and so your analysis will not be similarly partitioned. Instead, your document should be organized more like a true project report, and it should use the overall format:

- Simulation Study 1
- Simulation Study 2
- Simulation Study 3

Within each of the simulation studies, you should use the format:

- Introduction
- Methods
- Results
- Discussion

The **introduction** section should relay what you are attempting to accomplish. It should provide enough background to your work such that a reader would not need this directions document to understand what you are doing. (Basically, assume the reader is mostly familiar with the concepts from the course, but not this project.)

The **methods** section should contain the majority of your “work.” This section will contain the bulk of the `R` code that is used to generate the results. Your `R` code is not expected to be perfect idiomatic `R`, but it is expected to be understood by a reader without too much effort. Use RMarkdown and code comments to your advantage to explain your code if needed.

The **results** section should contain numerical or graphical summaries of your results as they pertain to the goal of each study.

The **discussion** section should contain discussion of your results. The discussion section should contain discussion of your results. Potential topics for discussion are suggested at the end of each simulation study section, but they are not meant to be an exhaustive list. These simulation studies are meant to be explorations into the principles of statistical modeling, so do not limit your responses to short, closed form answers as you do in homework assignments. Use the potential discussion questions as a starting point for your response.

- Your resulting `.html` file will be considered a self-contained “report,” which is the material that will determine the majority of your grade. Be sure to visibly include all `R` code and output that is *relevant*. (You should not include irrelevant code you tried that resulted in error or did not answer the question correctly.)
- Grading will be based on a combination of completing the required tasks, discussion of results, `R` usage, RMarkdown usage, and neatness and organization. For full details see the provided rubric.
- At the beginning of *each* of the three simulation studies, set a seed equal to your birthday, as is done on homework. (It should be the first code run for each study.) These should be the only three times you set a seed.

```{r}
birthday = 18760613
set.seed(birthday)
```

# Simulation Study 1: Significance of Regression

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, use $2000$ simulations. For each simulation, fit a regression model of the same form used to perform the simulation.

Use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Done correctly, you will have simulated the `y` vector $2 (models)×3 (sigmas)×2000 (sims)=12000$ times.

Potential discussions:

- Do we know the true distribution of any of these values?
- How do the empirical distributions from the simulations compare to the true distributions? (You could consider adding a curve for the true distributions if you know them.)
- How are each of the $F$ statistic, the p-value, and $R^2$ related to $\sigma$? Are any of those relationships the same for the significant and non-significant models?

Additional things to consider:

- Organize the plots in a grid for easy comparison.

$\underline{Introduction}$

This is the part i explain what i did.
We need to consider two models, "significant" model and "non-significant model". 
Difference being that $\beta_1$, $\beta_2$, $\beta_3$ are zero in the later.

The Generic Linear equation being 
\[ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i \]

**"Significant Model"**
\[ Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i \]

**"Non-Significant Model"**
\[ Y_i = \beta_0 + \epsilon_i \]

We create y_values from the Linear equations based on the above equations and
apply linear model and calculate the **"F-Statistics"**, **"P-Value"** and **"R2"**

We do the above calculations with our **2000** simulations with **3** different sigma
values **"1, 5, 10"** with a sample size of **25(n)**

$\underline{Method}$

Below we calculate the F-statistic, P-Value and R2 for **Significant model** with the
help of broom library.

```{r, message=FALSE, eval = TRUE, warning=FALSE}
library(broom)

#Significance Test
# sigma = 1, 5, 10
# Calculate F Statistic, p-value and R2

beta_0 = 3
beta_1 = 1
beta_2 = 1
beta_3 = 1

n = 25 #sample_size
study1 = read.csv("study_1.csv")
x1 = study1$x1
x2 = study1$x2
x3 = study1$x3
x0 = rep(1, n)
sig = 1 #1, 5, 10

X = cbind(x0, x1, x2, x3)
C = solve(t(X) %*% X)
y = rep(0, n)

sig = 1
num_sims = 2000 # RAJA FIXME
sim_data = data.frame(y, x1, x2, x3)
f_stat_1 = rep(0, num_sims)
p_val_1 = rep(0, num_sims)
r2_1 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
 sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_1[i] = glance(sim_data_lm)$statistic
 p_val_1[i] = glance(sim_data_lm)$p.value
 r2_1[i] = glance(sim_data_lm)$r.squared
}

sig = 5
f_stat_5 = rep(0, num_sims)
p_val_5 = rep(0, num_sims)
r2_5 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
 sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_5[i] = glance(sim_data_lm)$statistic
 p_val_5[i] = glance(sim_data_lm)$p.value
 r2_5[i] = glance(sim_data_lm)$r.squared
}

sig = 10
f_stat_10 = rep(0, num_sims)
p_val_10 = rep(0, num_sims)
r2_10 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
 sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_10[i] = glance(sim_data_lm)$statistic
 p_val_10[i] = glance(sim_data_lm)$p.value
 r2_10[i] = glance(sim_data_lm)$r.squared
}
```

Below we calculate the F-statistic, P-Value and R2 for **Non-Significant model** with the
help of broom library.

```{r, eval = TRUE, warning=FALSE}
#Non-Significance Test
beta_0 = 3
beta_1 = 0
beta_2 = 0
beta_3 = 0

n = 25 #sample_size
study1 = read.csv("study_1.csv")
x1 = study1$x1
x2 = study1$x2
x3 = study1$x3
x0 = rep(1, n)
sig = 1 #1, 5, 10

X = cbind(x0, x1, x2, x3)
C = solve(t(X) %*% X)
y = rep(0, n)

sig = 1
f_stat_ns_1 = rep(0, num_sims)
p_val_ns_1 = rep(0, num_sims)
r2_ns_1 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
   sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_ns_1[i] = glance(sim_data_lm)$statistic
 p_val_ns_1[i] = glance(sim_data_lm)$p.value
 r2_ns_1[i] = glance(sim_data_lm)$r.squared
}

sig = 5
f_stat_ns_5 = rep(0, num_sims)
p_val_ns_5 = rep(0, num_sims)
r2_ns_5 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
 sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_ns_5[i] = glance(sim_data_lm)$statistic
 p_val_ns_5[i] = glance(sim_data_lm)$p.value
 r2_ns_5[i] = glance(sim_data_lm)$r.squared
}

sig = 10
f_stat_ns_10 = rep(0, num_sims)
p_val_ns_10 = rep(0, num_sims)
r2_ns_10 = rep(0, num_sims)
for (i in 1:num_sims) {
 eps = rnorm(25, mean = 0, sd = sig)
 sim_data$y = beta_0 * x0 + beta_1 * x1 + beta_2 * x2 + beta_3 * x3 + eps
 sim_data_lm = lm(y ~ x1 + x2 + x3, data = sim_data)

 f_stat_ns_10[i] = glance(sim_data_lm)$statistic
 p_val_ns_10[i] = glance(sim_data_lm)$p.value
 r2_ns_10[i] = glance(sim_data_lm)$r.squared
}
```


$\underline{Results}$

We plot the histograms of f-statistic, p-value and R2 for Significant Model below.

``` {r}
#Plotting significance Model
par(mfrow=c(3,3))

hist(f_stat_1, prob = TRUE, breaks = 20, xlab = "Significance f-statistics for sigma 1", ylab = "value", main = "", border = "dodgerblue")
hist(p_val_1, prob = TRUE, breaks = 20, xlab = "Significance p-value for sigma 1", ylab = "value", main = "", border = "dodgerblue")
hist(r2_1, prob = TRUE, breaks = 20, xlab = "Significance r squared for sigma 1", ylab = "value", main = "", border = "dodgerblue")


hist(f_stat_5, prob = TRUE, breaks = 20, xlab = "Significance f-statistics for sigma 5", ylab = "value", main = "", border = "dodgerblue")
hist(p_val_5, prob = TRUE, breaks = 20, xlab = "Significance p-value for sigma 5", ylab = "value", main = "", border = "dodgerblue")
hist(r2_5, prob = TRUE, breaks = 20, xlab = "Significance r squared for sigma 5", ylab = "value", main = "", border = "dodgerblue")


hist(f_stat_10, prob = TRUE, breaks = 20, xlab = "Significance f-statistics for sigma 10", ylab = "value", main = "", border = "dodgerblue")
hist(p_val_10, prob = TRUE, breaks = 20, xlab = "Significance p-value for sigma 10", ylab = "value", main = "", border = "dodgerblue")
hist(r2_10, prob = TRUE, breaks = 20, xlab = "Significance r squared for sigma 10", ylab = "value", main = "", border = "dodgerblue")
```

We plot the histograms of f-statistic, p-value and R2 for Non-Significant Model below along with the curves for the distributions for comparison.


``` {r }
#Plotting Non-significance Model
par(mfrow=c(3,3))

hist(f_stat_ns_1, prob = TRUE, breaks = 20, xlab = "Non-Significance f-statistics for sigma 1", ylab = "value", main = "", border = "dodgerblue")
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), col = "red", add = TRUE, lwd = 2)
hist(p_val_ns_1, prob = TRUE, breaks = 20, xlab = "Non-Significance p-value for sigma 1", ylab = "value", main = "", border = "dodgerblue")
curve(dunif(x), col = "red", add = TRUE, lwd = 2)
hist(r2_ns_1, prob = TRUE, breaks = 20, xlab = "Non-Significance r squared for sigma 1", ylab = "value", main = "", border = "dodgerblue")
curve(dbeta(x, 4/2, (n-4-1)/2),  col = "red", add = TRUE, lwd = 2) # n = 25, p = 4

hist(f_stat_ns_5, prob = TRUE, breaks = 20, xlab = "Non-Significance f-statistics for sigma 5", ylab = "value", main = "", border = "dodgerblue")
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), col = "red", add = TRUE, lwd = 2)
hist(p_val_ns_5, prob = TRUE, breaks = 20, xlab = "Non-Significance p-value for sigma 5", ylab = "value", main = "", border = "dodgerblue")
curve(dunif(x), col = "red", add = TRUE, lwd = 2)
hist(r2_ns_5, prob = TRUE, breaks = 20, xlab = "Non-Significance r squared for sigma 5", ylab = "value", main = "", border = "dodgerblue")
curve(dbeta(x, 4/2, (n-4-1)/2),  col = "red", add = TRUE, lwd = 2) # n = 25, p = 4

hist(f_stat_ns_10, prob = TRUE, breaks = 20, xlab = "Non-Significance f-statistics for sigma 10", ylab = "value", main = "", border = "dodgerblue")
curve(df(x, df1 = 4 - 1, df2 = 25 - 4), col = "red", add = TRUE, lwd = 2)
hist(p_val_ns_10, prob = TRUE, breaks = 20, xlab = "Non-Significance p-value for sigma 10", ylab = "value", main = "", border = "dodgerblue")
curve(dunif(x), col = "red", add = TRUE, lwd = 2)
hist(r2_ns_10, prob = TRUE, breaks = 20, xlab = "Non-Significance r squared for sigma 10", ylab = "value", main = "", border = "dodgerblue")
curve(dbeta(x, 4/2, (n-4-1)/2),  col = "red", add = TRUE, lwd = 2) # n = 25, p = 4
```

$\underline{Discussion}$

**F-Statistic**
Significant model
a. We notice that F-Statistics follows a right skewed distribution.
b. We don't need to add a curve for the true distribution but the empirical 
distributions from the simulations ranges from 0 to 0.03.
c.The F-Statistics value increases with the increase in the sigma value.
This is an expected behavior as we are increase the Noise factor in the model.

Non-Significant model
a. F-Statistics empirical calculations pretty much follow the true distribution values.
b. We can notice from the equation that the factors that are influencing the f-stats
value is in the form of p-1 and n-p
c. We notice that with the increase of sigma value the histogram squeezes itself
closer to the mean value of the distribution.

**P-Value**
Significant model
a. P-Value follows a right skew distribution wit the increase of noise in the model.
With a sigma value of 1, the p-value distribution is close to the mean value.
b. We don't need to add true distribution curve.
c. we notice that the p-value increases with the increase in the sigma value. Meaning
that it values are getting distributed away from the mean, resulting is not so reliable
estimations.

Non-Significant Model
a. P-Value remains pretty much a constant value.
b. We also notice that p-value follows a uniform distribution (x).
c. Irrespective of the Sigma value as it is meant to be since the beta1 value is zero.

**R Squared**
Significant model
We notice that R2 increases with the increase in the Sigma Values. With an increase
in the sigma value, the R2 values also increase resulting a bigger range of variance for 
the esitmated values.

Non-Significant model
R2 follows a beta distribution with the parameters (p/2) and (n-p-1)/2.
As expected the R2 value increase with an increase in the sigma value because of 
the extra noise that is added in to the model.



# Simulation Study 2: Using RMSE for Selection?

In homework we saw how Test RMSE can be used to select the “best” model. In this simulation study we will investigate how well this procedure works. Since splitting the data is random, we don’t expect it to work correctly each time. We could get unlucky. But averaged over many attempts, we should expect it to select the appropriate model.

We will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

We will consider a sample size of $500$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

Use the data found in [`study_2.csv`](study_2.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Each time you simulate the data, randomly split the data into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

For each, fit **nine** models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

For each model, calculate Train and Test RMSE.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

Repeat this process with $1000$ simulations for each of the $3$ values of $\sigma$. For each value of $\sigma$, create a plot that shows how average Train RMSE and average Test RMSE changes as a function of model size. Also show the number of times the model of each size was chosen for each value of $\sigma$.

Done correctly, you will have simulated the $y$ vector $3×1000=3000$ times. You will have fit $9×3×1000=27000$ models. A minimal result would use $3$ plots. Additional plots may also be useful.

Potential discussions:

- Does the method **always** select the correct model? On average, does is select the correct model?
- How does the level of noise affect the results?

$\underline{Introduction}$

In this simulation study we look at the RSME calculations based on 
\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]
for a Train dataset and Test dataset extracted from the given dataset.

We created 9 models for the evaluating the datasets and come up with the results for each model
with a varying sigma value in the range of (1,2,4) and a sample data set of size 500 and 1000 simulations.

$\underline{Method}$

We use broom library to create the summary details of the a LM model.

``` {r}
library(broom)
birthday = 18760613
set.seed(birthday)

rmse = function(acutual, predicted) {
  sqrt(mean((acutual - predicted)^2))
}

beta0 = 0
beta1 = 3
beta2 = -4
beta3 = 1.6
beta4 = -1.1
beta5 = 0.7
beta6 = 0.5

n = 500
study2 = read.csv("study_2.csv")
x0 = rep(1, n)
x1 = study2$x1
x2 = study2$x2
x3 = study2$x3
x4 = study2$x4
x5 = study2$x5
x6 = study2$x6
x7 = study2$x7
x8 = study2$x8
x9 = study2$x9
C = cbind(x0, x1, x2, x3, x4, x5, x6, x7, x8, x9)

sig = 1

sigmas = c(1,2,4)

avg_rmse_train = matrix(0, nrow=3, ncol=9)
avg_rmse_tst = matrix(1, nrow=3, ncol=9)

for (s in 1:length(sigmas)) {
  
  eps = rnorm(n, mean = 0, sd = sigmas[s])
  study2$y = beta0 * x0 + beta1 * x1 + beta2 * x2 + beta3 * x3 + beta4 * x4 + beta5 * x5 + beta6 * x6 + eps
  
  num_sims = 1000 #1000 RAJA FIXME
  
  rmse_train_1 = rep(0, num_sims)
  rmse_train_2 = rep(0, num_sims)
  rmse_train_3 = rep(0, num_sims)
  rmse_train_4 = rep(0, num_sims)
  rmse_train_5 = rep(0, num_sims)
  rmse_train_6 = rep(0, num_sims)
  rmse_train_7 = rep(0, num_sims)
  rmse_train_8 = rep(0, num_sims)
  rmse_train_9 = rep(0, num_sims)
  rmse_tst_1 = rep(0, num_sims)
  rmse_tst_2 = rep(0, num_sims)
  rmse_tst_3 = rep(0, num_sims)
  rmse_tst_4 = rep(0, num_sims)
  rmse_tst_5 = rep(0, num_sims)
  rmse_tst_6 = rep(0, num_sims)
  rmse_tst_7 = rep(0, num_sims)
  rmse_tst_8 = rep(0, num_sims)
  rmse_tst_9 = rep(0, num_sims)

  for (i in 1:num_sims) {
    train_idx = sample(1:nrow(study2), 250)
    train_study2 = study2[train_idx, ]
    test_study2 = study2[-train_idx, ]
    
    model1 = lm(y ~ x1, data = train_study2)
    model2 = lm(y ~ x1 + x2, data = train_study2)
    model3 = lm(y ~ x1 + x2 + x3, data = train_study2)
    model4 = lm(y ~ x1 + x2 + x3 + x4, data = train_study2)
    model5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = train_study2)
    model6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = train_study2)
    model7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = train_study2)
    model8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = train_study2)
    model9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = train_study2)
    
    rmse_train_1[i] = rmse(train_study2$y, predict(model1, train_study2))
    rmse_train_2[i] = rmse(train_study2$y, predict(model2, train_study2))
    rmse_train_3[i] = rmse(train_study2$y, predict(model3, train_study2))
    rmse_train_4[i] = rmse(train_study2$y, predict(model4, train_study2))
    rmse_train_5[i] = rmse(train_study2$y, predict(model5, train_study2))
    rmse_train_6[i] = rmse(train_study2$y, predict(model6, train_study2))
    rmse_train_7[i] = rmse(train_study2$y, predict(model7, train_study2))
    rmse_train_8[i] = rmse(train_study2$y, predict(model8, train_study2))
    rmse_train_9[i] = rmse(train_study2$y, predict(model9, train_study2))
    
    rmse_tst_1[i] = rmse(test_study2$y, predict(model1, test_study2))
    rmse_tst_2[i] = rmse(test_study2$y, predict(model2, test_study2))
    rmse_tst_3[i] = rmse(test_study2$y, predict(model3, test_study2))
    rmse_tst_4[i] = rmse(test_study2$y, predict(model4, test_study2))
    rmse_tst_5[i] = rmse(test_study2$y, predict(model5, test_study2))
    rmse_tst_6[i] = rmse(test_study2$y, predict(model6, test_study2))
    rmse_tst_7[i] = rmse(test_study2$y, predict(model7, test_study2))
    rmse_tst_8[i] = rmse(test_study2$y, predict(model8, test_study2))
    rmse_tst_9[i] = rmse(test_study2$y, predict(model9, test_study2))
  }
  
  avg_rmse_train[s, 1] = mean(rmse_train_1)
  avg_rmse_train[s, 2] = mean(rmse_train_2)
  avg_rmse_train[s, 3] = mean(rmse_train_3)
  avg_rmse_train[s, 4] = mean(rmse_train_4)
  avg_rmse_train[s, 5] = mean(rmse_train_5)
  avg_rmse_train[s, 6] = mean(rmse_train_6)
  avg_rmse_train[s, 7] = mean(rmse_train_7)
  avg_rmse_train[s, 8] = mean(rmse_train_8)
  avg_rmse_train[s, 9] = mean(rmse_train_9)
  
  avg_rmse_tst[s, 1] = mean(rmse_tst_1)
  avg_rmse_tst[s, 2] = mean(rmse_tst_2)
  avg_rmse_tst[s, 3] = mean(rmse_tst_3)
  avg_rmse_tst[s, 4] = mean(rmse_tst_4)
  avg_rmse_tst[s, 5] = mean(rmse_tst_5)
  avg_rmse_tst[s, 6] = mean(rmse_tst_6)
  avg_rmse_tst[s, 7] = mean(rmse_tst_7)
  avg_rmse_tst[s, 8] = mean(rmse_tst_8)
  avg_rmse_tst[s, 9] = mean(rmse_tst_9)
}
```


$\underline{Results}$

Following is a resultant plot of average rmse train vs average rmse test per model 
with sigma value of 1

``` {r }
train_max = max(avg_rmse_train)
test_max = max(avg_rmse_tst)

par(mfrow=c(2,1))
model_num = c(1:9)
plot(avg_rmse_train[1, ] ~ model_num, xlim = c(1,9), ylim = c(0, train_max), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 1", ylab = "Avg Train RSME")
text(avg_rmse_train[1, ] ~ model_num, labels = round(avg_rmse_train[1, ], 3), pos = 3, cex = .8, col="blue")

plot(avg_rmse_tst[1, ] ~ model_num, xlim = c(1,9), ylim = c(0, test_max), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 1", ylab = "Avg Test RSME")
text(avg_rmse_tst[1, ] ~ model_num, labels = round(avg_rmse_tst[1, ], 3), pos = 3, cex = .8, col="green")

```
Following is a resultant plot of average rmse train vs average rmse test per model 
with sigma value of 2


``` {r }
train_max = max(avg_rmse_train)
test_max = max(avg_rmse_tst)

par(mfrow=c(2,1))
model_num = c(1:9)
plot(avg_rmse_train[2, ] ~ model_num, xlim = c(1,9), ylim = c(0, train_max), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 2", ylab = "Avg Train RSME")
text(avg_rmse_train[2, ] ~ model_num, labels = round(avg_rmse_train[2, ], 3), pos = 3, cex = .8, col="blue")

plot(avg_rmse_tst[2, ] ~ model_num, xlim = c(1,9), ylim = c(0, test_max), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 2", ylab = "Avg Test RSME")
text(avg_rmse_tst[2, ] ~ model_num, labels = round(avg_rmse_tst[2, ], 3), pos = 3, cex = .8, col="green")

```

Following is a resultant plot of average rmse train vs average rmse test per model 
with sigma value of 4

``` {r }
train_max = max(avg_rmse_train)
test_max = max(avg_rmse_tst)

par(mfrow=c(2,1))
model_num = c(1:9)
plot(avg_rmse_train[3, ] ~ model_num, xlim = c(1,9), ylim = c(0, train_max+1), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 4", ylab = "Avg Train RSME")
text(avg_rmse_train[3, ] ~ model_num, labels = round(avg_rmse_train[3, ], 3), pos = 1, cex = .8, col="blue")

plot(avg_rmse_tst[3, ] ~ model_num, xlim = c(1,9), ylim = c(0, test_max+1), pch = 20, cex = 2, col="red", xlab = "Model Numbers with sigma 4", ylab = "Avg Test RSME")
text(avg_rmse_tst[3, ] ~ model_num, labels = round(avg_rmse_tst[3, ], 3), pos = 1, cex = .8, col="green")

```


$\underline{Discussion}$

RMSE computes the root mean squared error between two numeric vectors. In our we 
we are calculating RMSE between 9 difference models that uses x1 to x9 predictors
and beta 1 to beta 5 parameters.

We have noticed that with the increase in the Sigma values the RSME values decreases,
explaining the influence of Noise/Sigma on the estimations calculations.

We also notice that Model 6 seems to be holding a minimum value in comparison with
models 7, 8 and 9, even though we use more predictors in the LR for Model7, Model8
and Model9.

This suggests that it is sometimes possible to get better results with less predictors than
more predictors in the LR model.



# Simulation Study 3: Power

In this simulation study we will investigate the **power** of the significance of regression test for simple linear regression. 

\[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

Recall, we had defined the *significance* level, $\alpha$, to be the probability of a Type I error.

\[
\alpha = P[\text{Reject } H_0 \mid H_0 \text{ True}] = P[\text{Type I Error}]
\]

Similarly, the probability of a Type II error is often denoted using $\beta$; however, this should not be confused with a regression parameter.

\[
\beta = P[\text{Fail to Reject } H_0 \mid H_1 \text{ True}] = P[\text{Type II Error}]
\]

*Power* is the probability of rejecting the null hypothesis when the null is not true, that is, the alternative is true and $\beta_{1}$ is non-zero.

\[
\text{Power} = 1 - \beta = P[\text{Reject } H_0 \mid H_1 \text{ True}]
\]

Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

Use the following code to generate the predictor values, `x`: values for different sample sizes.

```{r eval=FALSE}
x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination, simulate from the true model at least $1000$ times. Each time, perform the significance of the regression test. To estimate the power with these simulations, and some $\alpha$, use

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

It is *possible* to derive an expression for power mathematically, but often this is difficult, so instead, we rely on simulation.

Create three plots, one for each value of $\sigma$. Within each of these plots, add a “power curve” for each value of $n$ that shows how power is affected by signal strength, $\beta_1$.

Potential discussions:

- How do $n$, $\beta_1$, and $\sigma$ affect power? Consider additional plots to demonstrate these effects.
- Are $1000$ simulations sufficient?

**\underline{Introduction}**

In this simulation we investigate the power of significance of regression test for SLR \[
H_0: \beta_{1} = 0 \ \text{vs} \ H_1: \beta_{1} \neq 0
\]

We use the following Linear Equation for modelling
\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

where $\beta_0 = 0$, and so only $\beta_1$ is controlling the amount of "signal." 

Following different signals, noises, and sample sizes are used for simulation study:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

Also alpha value is fixed at $\alpha = 0.05$.


**\underline{Methods}**

We create a function calc_power to calculate the power estimated for a particular
sigma and sample_size values for all the beta1 ranges.
``` {r}
birthday = 18760613
set.seed(birthday)

beta0 = 0
beta1 = seq(-2.0, 2, by=0.1) #41 in size
sig = 1 #1, 2, 3
n = 10 #10, 20, 30
alpha = 0.05

num_sims = 1000 #RAJA FIXME
# for every beta1 value
#  for every simulations num_sims
calc_power = function(sig = 1, n = 10) {
  power = rep(0, length(beta1))
  for (i in 1: length(beta1)) {
    p_values = rep(0, num_sims) #1000 reps
    for (noOfSims in 1:num_sims) {
      eps = rnorm(n, mean = 0, sd = sig)
      y = beta0 + beta1[i] * x_values + eps #41 in size
      m_lm = lm(y ~ x_values, data = data.frame(y, x=x_values))
      p_values[noOfSims] = glance(m_lm)$p.value
    }
    power[i] = mean(p_values < alpha)
  }
  return(power)
}


#power_sigma_n
n = 10
x_values = seq(0, 5, length = n) #n in size
power_1_10 = calc_power(1, n)
power_2_10 = calc_power(2, n)
power_4_10 = calc_power(4, n)

n = 20
x_values = seq(0, 5, length = n) #n in size
power_1_20 = calc_power(1, n)
power_2_20 = calc_power(2, n)
power_4_20 = calc_power(4, n)

n = 30
x_values = seq(0, 5, length = n) #n in size
power_1_30 = calc_power(1, n)
power_2_30 = calc_power(2, n)
power_4_30 = calc_power(4, n)

```

**\underline{Results}**

The results are plotted as shown below where red is for sample size 10, green for 
sample sizes 20 and blue for sample sizes 30 with varying sigma values.


``` {r}
par(mfrow=c(1,3))
plot(power ~ beta, data = data.frame(power = power_1_10, beta = beta1), xlab = "beta1 values", ylab = "power", pch = 20, cex = 2, type="l", col="red")
# lines(beta1, power_1_10, col = "red", lwd=2, type="b")
lines(beta1, power_1_20, col = "green", lwd=2, type="b")
lines(beta1, power_1_30, col = "blue", lwd=2, type="b")
title("n, beta1 and sigma(1) impact on Power ")
legend("bottomright", c("Sigma=1, N=10", "sigma=1, N=20", "sigma=1, N=30"), lwd = 2, col=c("red","green", "blue"))

plot(power ~ beta, data = data.frame(power = power_2_10, beta = beta1), xlab = "beta1 values", ylab = "power", pch = 20, cex = 2, type="l", col="red")
# lines(beta1, power_2_10, col = "red", lwd=2, type="b")
lines(beta1, power_2_20, col = "green", lwd=2, type="b")
lines(beta1, power_2_30, col = "blue", lwd=2, type="b")
title("n, beta1 and sigma(2) impact on Power ")
legend("bottomright", c("Sigma=2, N=10", "sigma=2, N=20", "sigma=2, N=30"), lwd = 2, col=c("red","green", "blue"))

plot(power ~ beta, data = data.frame(power = power_4_10, beta = beta1), xlab = "beta1 values", ylab = "power", pch = 20, cex = 2, type="l", col="red")
#lines(beta1, power_4_10, col = "red", lwd=2, type="b")
lines(beta1, power_4_20, col = "green", lwd=2, type="b")
lines(beta1, power_4_30, col = "blue", lwd=2, type="b")
title("n, beta1 and sigma(1) impact on Power ")
legend("bottomright", c("Sigma=4, N=10", "sigma=4, N=20", "sigma=4, N=30"), lwd = 2, col=c("red","green", "blue"))

```

**Discussions**

Power is defined as the Probability that test will reject a null hypothesis correctly 
when the alternative is true.

We notice that with an increase in the sigma value the R,G,B lines part apart.
But with an increase in the number of simulations they come closer.
The beta1 values play a significant role in relating the R,G,B lines as at a value of
zero the power comes down to 0.

Is 1000 Simulations enough for the evalutions. Yes, i beliver so since we are getting
good results (R,G,B lines coming closer). Although more number of simulations yields
more better results.

