{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS445: Computational Photography\n",
    "## Programming Project 4: Image-Based Lighting\n",
    "### Due Date: 11:59pm on Wednesday, Apr. 8, 2020\n",
    "\n",
    "This is a template solution file. \n",
    "Please feel free to use this for the base of your report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering HDR Radiance Maps (55 pts)\n",
    "\n",
    "We start by loading in necessary libraries used for this section of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter extension that allows reloading functions from imports without clearing kernel :D\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from os import path\n",
    "import math\n",
    "\n",
    "# Third-Party Imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# local imports\n",
    "from utils.io import read_image, write_image, read_hdr_image, write_hdr_image\n",
    "from utils.display import display_hdr_image_linear, display_hdr_image, display_log_irradiances\n",
    "from utils.hdr_helpers import gsolve\n",
    "from utils.hdr_helpers import get_equirectangular_image\n",
    "from utils.bilateral_filter import bilateral_filter\n",
    "from utils.meta import TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data collection (15 points)\n",
    "\n",
    "1. Find a good scene to photograph. The scene should have a flat surface to place your spherical mirror on. Either indoors or outdoors will work.\n",
    "<img src=\"images/project_4/background.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "2. Find a fixed, rigid spot to place your camera. A tripod is best, but you can get away with less. I used the back of a chair to steady my phone when taking my images.\n",
    "3. Place your spherical mirror on a flat surface, and make sure it doesn't roll by placing a cloth/bottle cap/etc under it. Make sure the sphere is not too far away from the camera -- it should occupy at least a 256x256 block of pixels.\n",
    "<img src=\"images/project_4/background_with_ball.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "4. Photograph the spherical mirror using at least three different exposure times. Make sure the camera does not move too much (slight variations are OK, but the viewpoint should generally be fixed). For best results, your exposure times should be at least 4 times longer and 4 times shorter (Â±2 stops) than your mid-level exposure (e.g. if your mid-level exposure time is 1/40s, then you should have at least exposure timess of 1/10s and 1/160s; the greater the range the better). Make sure to record the exposure times.\n",
    "5. Remove the mirror from the scene, and from the same viewpoint as the other photographs, take another picture of the scene at a normal exposure level (most pixels are neither over- or under-exposed). This will be the image that you will use for object insertion/compositing (the \"background\" image).\n",
    "6. After you copy all of the images from your camera/phone to your computer, load the spherical mirror images (from step 4) into your favorite image editor and crop them down to contain only the sphere.\n",
    "<img src=\"images/project_4/cropped.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "7. Small alignment errors may occur (due to camera motion or cropping). One way to fix these is through various alignment procedures, but for this project, we won't worry about these errors. If there are substantial differences in camera position/rotation among the set of images, re-take the photographs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace this with actual file path!\n",
    "low_exposure_mirror_ball_image_file = TODO()\n",
    "mid_exposure_mirror_ball_image_file = TODO()\n",
    "high_exposure_mirror_ball_image_file = TODO()\n",
    "background_image_file = TODO()\n",
    "\n",
    "\n",
    "# TODO: Extract exposure values for each images\n",
    "low_exposure = TODO()\n",
    "mid_exposure = TODO()\n",
    "high_exposure = TODO()\n",
    "\n",
    "\n",
    "\n",
    "# These images will be used for LDR mergings\n",
    "low_exposure_mirror_ball_image = read_image(low_exposure_mirror_ball_image_file)\n",
    "mid_exposure_mirror_ball_image = read_image(mid_exposure_mirror_ball_image_file)\n",
    "high_exposure_mirror_ball_image = read_image(high_exposure_mirror_ball_image_file)\n",
    "background_image = read_image(background_image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize mirror ball images:\n",
    "# TODO: change size of N to your favorite value!\n",
    "N = TODO()\n",
    "low_exposure_mirror_ball_image = cv2.resize(low_exposure_mirror_ball_image, (N, N))\n",
    "mid_exposure_mirror_ball_image = cv2.resize(mid_exposure_mirror_ball_image, (N, N))\n",
    "high_exposure_mirror_ball_image = cv2.resize(high_exposure_mirror_ball_image, (N, N))\n",
    "\n",
    "\n",
    "ldr_images = np.stack((low_exposure_mirror_ball_image, \n",
    "                       mid_exposure_mirror_ball_image, \n",
    "                       high_exposure_mirror_ball_image))\n",
    "exposures = [low_exposure, mid_exposure, high_exposure]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive LDR merging (10 points)\n",
    "\n",
    "After collecting data, load the cropped images, and resize them to all be square and the same dimensions (e.g. cv2.resize(ldr,(N,N)) N is the new size). Either find the exposure times using the EXIF data (usually accessible in the image properties, or via matlab's imfinfo), or refer to your recorded exposure times. To put the images in the same intensity domain, divide each by its exposure time (e.g. ldr1_scaled = ldr1 / exposure_time1). After this conversion, all pixels will be scaled to their approximate value if they had been exposed for 1s.\n",
    "\n",
    "The easiest way to convert your scaled LDR images to an HDR is simply to average them. Create one of these for comparison to your later results. \n",
    "\n",
    "To save the HDR image, use given write_hdr_image function.\n",
    "To visualize HDR image, use given display_hdr_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_hdr_naive(ldr_images: np.ndarray, exposures: list) -> (np.ndarray, np.ndarray):\n",
    "    '''\n",
    "    Makes HDR image using multiple LDR images, and its corresponding exposure values.\n",
    "    \n",
    "    The steps to implement:\n",
    "    1) Divide each images by its exposure time.\n",
    "        - This will rescale images as if it has been exposed for 1 second.\n",
    "    \n",
    "    2) Return average of above images\n",
    "    \n",
    "    \n",
    "    For further explanation, please refer to problem page for how to do it.\n",
    "      \n",
    "    Args:\n",
    "        ldr_images(np.ndarray): N x H x W x 3 shaped numpy array representing\n",
    "            N ldr images with width W, height H, and channel size of 3 (RGB)\n",
    "        exposures(list): list of length N, representing exposures of each images.\n",
    "            Each exposure should correspond to LDR images' exposure value.\n",
    "    Return:\n",
    "        (np.ndarray): H x W x 3 shaped numpy array representing HDR image merged using\n",
    "            naive ldr merging implementation.\n",
    "        (np.ndarray): N x H x W x 3 shaped numpy array represending log irradiances\n",
    "            for each exposures\n",
    "            \n",
    "    '''\n",
    "    N, H, W, C = ldr_images.shape\n",
    "    # sanity check\n",
    "    assert N == len(exposures)\n",
    "    \n",
    "    # TODO: Implement ldr_images + exposures -> HDR image function here\n",
    "    # np_exposures = N, shaped array\n",
    "    hdr_image = TODO()\n",
    "    log_irradiances = TODO()\n",
    "    \n",
    "    return hdr_image, log_irradiances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HDR image, log irradiance\n",
    "naive_hdr_image, naive_log_irradiances = make_hdr_naive(ldr_images, exposures)\n",
    "\n",
    "# write HDR image to directory\n",
    "write_hdr_image(naive_hdr_image, 'images/outputs/naive_hdr.hdr')\n",
    "\n",
    "# display HDR image\n",
    "display_hdr_image(naive_hdr_image)\n",
    "\n",
    "# display log irradiance image\n",
    "display_log_irradiances(naive_log_irradiances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDR merging without under- and over-exposed regions (10 points)\n",
    "\n",
    "The naive method has an obvious limitation: if any pixels are under- or over-exposed, the result will contain clipped (and thus incorrect) information. A simple fix is to find these regions (e.g. a pixel might be considered over exposed if its value is less than 0.02 or greater than 0.98, assuming [0,1] images), and exclude them from the averaging process. Another way to think about this is that the naive method is extended using a weighted averaging procedure, where weights are 0 if the pixel is over/under-exposed, and 1 otherwise. Note that with this method, it might be the case that for a given pixel it is never properly exposed (i.e. always either above or below the threshold in each exposure). \n",
    "\n",
    "There are perhaps better methods that achieve similar results but don't require a binary weighting. For example, we could create a weighting function that is small if the input (pixel value) is small or large, and large otherwise, and use this to produce an HDR image. In python, such a function can be created with: \n",
    "\n",
    "w = lambda z: float(128-abs(z-128))\n",
    "\n",
    "assuming pixel values range in [0,255].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hdr_filtered(ldr_images: np.ndarray, exposures: list) -> (np.ndarray, np.ndarray):\n",
    "    '''\n",
    "    Makes HDR image using multiple LDR images, and its corresponding exposure values.\n",
    "    Please refer to problem notebook for how to do it.\n",
    "    \n",
    "    The steps to implement:\n",
    "    1) compute weights for images with based on intensities for each exposures\n",
    "        - This can be a binary mask to exclude low / high intensity values\n",
    "\n",
    "    2) Divide each images by its exposure time.\n",
    "        - This will rescale images as if it has been exposed for 1 second.\n",
    "    \n",
    "    3) Return weighted average of above images\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        ldr_images(np.ndarray): N x H x W x 3 shaped numpy array representing\n",
    "            N ldr images with width W, height H, and channel size of 3 (RGB)\n",
    "        exposures(list): list of length N, representing exposures of each images.\n",
    "            Each exposure should correspond to LDR images' exposure value.\n",
    "    Return:\n",
    "        (np.ndarray): H x W x 3 shaped numpy array representing HDR image merged without\n",
    "            under - over exposed regions\n",
    "        (np.ndarray): N x H x W x 3 shaped numpy array represending log irradiances\n",
    "            for each exposures\n",
    "    '''\n",
    "    N, H, W, C = ldr_images.shape\n",
    "    # sanity check\n",
    "    assert N == len(exposures)\n",
    "    \n",
    "    # TODO: Implement ldr_images + exposures -> HDR image function here\n",
    "    hdr_image = TODO()\n",
    "    log_irradiances = TODO()\n",
    "    \n",
    "    return hdr_image, log_irradiances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HDR image, log irradiance\n",
    "filtered_hdr_image, filtered_log_irradiances = make_hdr_filtered(ldr_images, exposures)\n",
    "\n",
    "# write HDR image to directory\n",
    "write_hdr_image(filtered_hdr_image, 'images/outputs/filtered_hdr.hdr')\n",
    "\n",
    "# display HDR image\n",
    "display_hdr_image(filtered_hdr_image)\n",
    "\n",
    "# display log irradiance image\n",
    "display_log_irradiances(naive_log_irradiances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDR merging and response function estimation (20 points)\n",
    "\n",
    "Nearly all cameras apply a non-linear function to recorded raw pixel values in order to better simulate human vision. In other words, the light incoming to the camera (radiance) is recorded by the sensor, and then mapped to a new value by this function. This function is called the film response function, and in order to convert pixel values to true radiance values, we need to estimate this response function. Typically the response function is hard to estimate, but since we have multiple observations at each pixel at different exposures, we can do a reasonable job up to a missing constant. \n",
    "\n",
    "The method we will use to estimate the response function is outlined in this paper. Given pixel values Z at varying exposure times t, the goal is to solve for g(Z) = ln(R*t) = ln(R)+ln(t). This boils down to solving for R (irradiance) since all other variables are known. By these definitions, g is the inverse, log response function. The paper provides code to solve for g given a set of pixels at varying exposures (we also provide gsolve in utils). Use this code to estimate g for each image channel (r/g/b). Then, recover the HDR image using equation 6 in the paper. \n",
    "\n",
    "#### Some hints on using gsolve:\n",
    "  - When providing input to gsolve, don't use all available pixels, otherwise you will likely run out of memory / have very slow run times. To overcome, just randomly sample a set of pixels (100 or so can suffice), but make sure all pixel locations are the same for each exposure.\n",
    "  - The weighting function w should be implemented using Eq. 4 from the paper (this is the same function that can be used for the previous LDR merging method, i.e. w = lambda z: float(128-abs(z-128))).\n",
    "  - Try different lambda values for recovering g. Try lambda=1 initially, then solve for g and plot it. It should be smooth and continuously increasing. If lambda is too small, g will be bumpy.\n",
    "  - Refer to Eq. 6 in the paper for using g and combining all of your exposures into a final image. Note that this produces log radiance values, so make sure to exponentiate the result and save absolute radiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hdr_estimation(ldr_images: np.ndarray, exposures: list)-> (np.ndarray, np.ndarray):\n",
    "    '''\n",
    "    Makes HDR image using multiple LDR images, and its corresponding exposure values.\n",
    "    Please refer to problem notebook for how to do it.\n",
    "    \n",
    "    **IMPORTANT**\n",
    "    The gsolve operations should be ran with:\n",
    "        Z: int64 array of shape N x P, where N = number of images, P = number of pixels\n",
    "        B: float32 array of shape N,\n",
    "        l: Number\n",
    "        W: function that takes int and returns float\n",
    "    \n",
    "    The steps to implement:\n",
    "    1) Create random points to sample (from mirror ball region)\n",
    "    2) For each exposures, compute g values using samples\n",
    "    3) Recover HDR image using g values\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        ldr_images(np.ndarray): N x H x W x 3 shaped numpy array representing\n",
    "            N ldr images with width W, height H, and channel size of 3 (RGB)\n",
    "        exposures(list): list of length N, representing exposures of each images.\n",
    "            Each exposure should correspond to LDR images' exposure value.\n",
    "    Return:\n",
    "        (np.ndarray): H x W x 3 shaped numpy array representing HDR image merged using\n",
    "            gsolve\n",
    "        (np.ndarray): N x H x W x 3 shaped numpy array represending log irradiances\n",
    "            for each exposures\n",
    "        (np.ndarray): 3 x 256 shaped numpy array represending g values of each pixel intensities\n",
    "            at each channels (used for plotting)\n",
    "    '''\n",
    "    N, H, W, C = ldr_images.shape\n",
    "    # sanity check\n",
    "    assert N == len(exposures)\n",
    "    \n",
    "    # implement HDR estimation using gsolve\n",
    "    hdr_image = TODO()\n",
    "    log_irradiances = TODO()\n",
    "    g_per_exposures = TODO()\n",
    "    return hdr_image, log_irradiances, g_per_exposures\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get HDR image, log irradiance\n",
    "estimated_hdr_image, estimated_log_irradiance, estimated_g = make_hdr_estimation(ldr_images, exposures)\n",
    "\n",
    "# write HDR image to directory\n",
    "write_hdr_image(estimated_hdr_image, 'images/outputs/estimated_hdr.hdr')\n",
    "\n",
    "# display HDR image\n",
    "display_hdr_image(estimated_hdr_image)\n",
    "\n",
    "# display log irradiance image\n",
    "display_log_irradiances(estimated_log_irradiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display G function for each intensity values\n",
    "N, NG = estimated_g.shape\n",
    "labels = ['R', 'G', 'B']\n",
    "plt.figure()\n",
    "for n in range(N):\n",
    "    plt.plot(range(NG), estimated_g[n], label=labels[n])\n",
    "plt.gca().legend(('R', 'G', 'B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoramic transformations\n",
    "\n",
    "Now that we have an HDR image of the spherical mirror, we'd like to use it for relighting (i.e. image-based lighting). However, many programs don't accept the \"mirror ball\" format, so we need to convert it to a different 360 degree, panoramic format (there is a nice overview of many of these formats here). For this part of the project, you should implement the mirror ball to equirectangular (latitude longitude) transformation. Most rendering software accepts this format, including Blender's Cycles renderer, which is what we'll use in the next part of the project.\n",
    "\n",
    "To perform the transformation, you need to figure out the mapping between the mirrored sphere domain and the equirectangular domain. Hint: calculate the normals of the sphere (N) and assume the viewing direction (V) is constant. You can calculate reflection vectors with R = V - 2 * dot(V,N) * N, (NOTE that you'd have to implement channel-wise dot product). which is the direction that light is incoming from the world to the camera after bouncing off the sphere. The reflection vectors can then be converted to, providing the latitude and longitude (phi and theta) of the given pixel (fixing the distance to the origin, r, to be 1). Note that this assumes an orthographic camera (which is a close approximation as long as the sphere isn't too close to the camera). The view vector is assumed to be at (0,0,-1)\n",
    "\n",
    "Next, the equirectangular domain can be created by making an image in which the rows correspond to theta and columns correspond to phi in spherical coordinates. For this we have provided you a function in the starter code get_equirectangular_image() (You can find the function under utils in hdr_helpers.py). The function takes reflection vectors and the HDR image produced in the Naive implementation as input and returns the equirectangular image as output.\n",
    "\n",
    "Below is an example transformation.\n",
    "\n",
    "<img src=\"images/project_4/panoramic_trans.jpg\" width=\"500px\">\n",
    "\n",
    "Note that by choosing 360 as EH and 720 as EW, we are making every pixel in equirectangular image to correspond to area occupied by 0.5 degree x 0.5 degree in spherical coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_mirrorball_image = read_hdr_image('images/outputs/estimated_hdr.hdr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panoramic_transform(hdr_image):\n",
    "    '''\n",
    "    Given HDR mirror ball image, \n",
    "    \n",
    "    Expects mirror ball image to have center of the ball at center of the image, and\n",
    "    width and height of the image to be equal.\n",
    "    \n",
    "    Steps to implement:\n",
    "    1) Compute normal vector from mirror ball\n",
    "    2) Compute reflection vector of mirror ball using given equation\n",
    "    3) Map reflection vectors into spherical coordinates\n",
    "    4) Interpolate spherical coordinate values into equirectangular grid.\n",
    "      - hint: use scipy.interpolate.griddata\n",
    "\n",
    "\n",
    "    '''\n",
    "    H, W, C = hdr_image.shape\n",
    "    assert H == W\n",
    "    assert H % 2 == 0\n",
    "    assert C == 3\n",
    "    R = H // 2\n",
    "    \n",
    "    equirectangular_image = get_equirectangular_image(R, hdr_image)\n",
    "    return equirectangular_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_image = panoramic_transform(naive_hdr_image)\n",
    "display_hdr_image(eq_image)\n",
    "write_hdr_image(eq_image, 'images/outputs/equirectangular.hdr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering synthetic objects into photographs (35 pts)\n",
    "\n",
    "Next, we will use our equirectangular HDR image as an image-based light, and insert 3D objects into the scene. This consists of 3 main parts: modeling the scene, rendering, and compositing. Specific instructions follow below; if interested, see additional details in Debevec's paper. \n",
    "\n",
    "Begin by downloading/installing the Blender. This template assumes that you have version 2.8 of the blender; if you are using version 2.7x, please refer to project webpage for detailed steps. The course webpage has tutorial with sample blend file, while this tutorial assumes that you create your own blend file from scratch. Please right click, open in new tab to view GIFs in full size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the scene\n",
    "\n",
    "To insert objects, we must have some idea of the geometry and surface properties of the scene, as well as the lighting information that we captured in previous stages. In this step, you will manually create rough scene geometry/materials using Blender.\n",
    "\n",
    "First clear out initial scene including sample mesh and lighting.\n",
    "<img src=\"images/project_4/1_delete_initial.gif\" width=\"720px\">\n",
    "\n",
    "Next, set camera to correct position\n",
    "<img src=\"images/project_4/2_set_camera_position.gif\" width=\"720px\">\n",
    "\n",
    "Next, go into perspective camera mode.\n",
    "<img src=\"images/project_4/3_go_into_camera.gif\" width=\"720px\">\n",
    "\n",
    "Then, setup viewport dimension. You want the same dimension as your background image for this.\n",
    "<img src=\"images/project_4/4_setup_dimensions.gif\" width=\"720px\">\n",
    "\n",
    "\n",
    "We then want to load background image onto this viewport. Click on 'g' to move background image around, and 's' to resize it.\n",
    "<img src=\"images/project_4/5_load_background_and_align.gif\" width=\"720px\">\n",
    "\n",
    "Then, we want to add local scene. That is, add simple geometry (usually planes suffice) to recreate the geometry in the scene near where you'd like to insert objects. For best results, this should be close to where you placed the spherical mirror. Feel free to use the sample scene provided and move the vertices of the plane to match the surface you'd like to recreate (ignore the inserted bunny/teapot/etc for now). Once you're happy with the placement, add materials to the local scene: select a piece of local scene geometry, go to Properties->Materials, add a Diffuse BSDF material, and change the \"Color\" to roughly match the color from the photograph.\n",
    "<img src=\"images/project_4/6_adding_plane.gif\" width=\"720px\">\n",
    "<img src=\"images/project_4/7_setting_plane_material.gif\" width=\"720px\">\n",
    "Insert synthetic objects into the scene. Feel free to use the standard models that I've included in the sample blend file, or find your own (e.g. Turbosquid, Google 3D Warehouse, DModelz, etc). Add interesting materials to your inserted objects as well. This tutorial is a great introduction to creating materials in Blender. Once finished, your scene should now look something like the right image below.\n",
    "<img src=\"images/project_4/8_adding_mesh.gif\" width=\"720px\">\n",
    "<img src=\"images/project_4/9_set_mesh_material.gif\" width=\"720px\">\n",
    "Then, add your HDR image (the equirectangular map made above) to the scene.\n",
    "\n",
    "First, use notebook to save the HDR panorama: write_hdr_image(eq_image, 'equirectangular.hdr'). \n",
    "\n",
    "In the World tab on property panel, make sure Surface=\"Background\" and Color=\"Environment Texture\". Locate your saved HDR image in the filename field below \"Environment Texture\".\n",
    "<img src=\"images/project_4/10_add_equirectangular_lighting.gif\" width=\"720px\">\n",
    "\n",
    "Finally, render scene using 'cycle' renderer. Note that this step takes a while to run.\n",
    "<img src=\"images/project_4/11_render_cycles.gif\" width=\"720px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished rendering scene with object, you should save it using image > save functionality in render tab.\n",
    "\n",
    "\n",
    "You also need to create 'empty' scene without inserted objects, and mask for added objects.\n",
    "To render masked scene, remove equirectangular based lighting, and set all inserted objects' material to emission.\n",
    "You can render the mask by using 'Eevee' renderer\n",
    "\n",
    "\n",
    "<img src=\"images/project_4/12_save_mask.gif\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all rendered images, you should have there three images, (rendered with object, without object, and mask):\n",
    "\n",
    "<img src=\"images/project_4/render_render.jpg\" width=\"200px\" style=\"display: inline\">\n",
    "<img src=\"images/project_4/render_empty.jpg\" width=\"200px\" style=\"display: inline\">\n",
    "<img src=\"images/project_4/render_mask.jpg\" width=\"200px\" style=\"display: inline\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part assumes that you have these from Blender.\n",
    "blender_output_with_object_path = TODO()\n",
    "blender_output_without_object_path = TODO()\n",
    "blender_output_mask_path = TODO()\n",
    "\n",
    "O = read_image(blender_output_with_object_path)\n",
    "E = read_image(blender_output_without_object_path)\n",
    "M = read_image(blender_output_mask_path)\n",
    "M = M > 0.5\n",
    "I = background_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = TODO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_image(merged, 'images/outputs/merged.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bells & Whistles (Extra Points)\n",
    "\n",
    "### Additional Image-Based Lighting Result (20 pts)\n",
    "Give an image-based lighting result with new objects with the same HDR light map (10 points). Compositing a result with a new HDR light map (10 more points). There are a total of 20 possible points here. \n",
    "\n",
    "### Other panoramic transformations (20 pts) \n",
    "Different software accept different spherical HDR projections. In the main project, we've converted from the mirror ball format to the equirectangular format. There are also two other common formats: angular and vertical cross (examples here). Implement these transformations for 10 extra points each (20 possible).\n",
    "\n",
    "### Photographer/tripod removal (20 pts) \n",
    "If you look closely at your mirror ball images, you'll notice that the photographer (you) and/or your tripod is visible, and probably occupies up a decent sized portion of the mirror's reflection. For 20 extra points, implement one of the following methods to remove the photographer: \n",
    "  1. Cut out the photographer and use in-painting/hole-filling to fill in the hole with background pixels (similar to the bells and whistles from Project 2), or \n",
    "  2. Use Debevec's method for removing the photographer (outlined here, steps 3-5; feel free to use Debevec's HDRShop for doing the panoramic rotations/blending). \n",
    "\n",
    "The second option works better, but requires you to create an HDR mirror ball image from two different viewpoints, and then merge them together using blending and panoramic rotations.\n",
    "\n",
    "### Local tonemapping operator (30 pts) \n",
    "HDR images can also be used to create hyper-realistic and contrast enhanced LDR images. This paper describes a simple technique for increasing the contrast of images by using a local tonemapping operator, which effectively compresses the photo's dynamic range into a displayable format while still preserving detail and contrast. For 30 extra credit points, implement the method found in the paper and compare your results to other tonemapping operations (see example below for ideas). You can use bilateral_filter code, provided by us, in your implementation, but do not use any other third party code. You can find some example HDR images here, including the memorial church image used below.\n",
    "\n",
    "<img src=\"images/project_4/local_tonemap.jpg\" width=\"800px\">\n",
    "From left to right: simple rescaling, rescaling+gamma correction, local tonemapping operator, local tonemapping+gamma correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
